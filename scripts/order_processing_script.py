import pandas as pd
import os
import glob
import shutil
from datetime import datetime
try:
    from config import (
        INPUT_DIR, OUTPUT_DIR, ARCHIVE_DIR, OUTPUT_CSV_PATH, ORPHAN_CSV_PATH,
        COLUMN_MAPPING, FINAL_COLUMN_ORDER
    )
except ImportError:
    print("âŒ éŒ¯èª¤ï¼šç„¡æ³•å¾ config.py å°å…¥è¨­å®šã€‚")
    exit()
import logging
import traceback

# --- æ—¥èªŒè¨­å®š ---
log_file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'python_script_log.txt'))
logging.basicConfig(
    filename=log_file_path,
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8',
    filemode='w'
)

# --- æ ¸å¿ƒè™•ç†å‡½å¼ ---

def create_robust_composite_key(df):
    """å»ºç«‹åŸºæ–¼è¨‚å–®æ—¥æœŸ + è¨‚å–®ç·¨è™Ÿ + è²·å®¶å¸³è™Ÿçš„ç©©å¥è¤‡åˆä¸»éµã€‚
    ä»¥æ•´ç­†è¨‚å–®ç‚ºå–®ä½é€²è¡Œæ¯”å°ï¼Œæ–°è³‡æ–™æœƒå®Œå…¨è¦†è“‹èˆŠè³‡æ–™ã€‚"""
    
    # ç¢ºä¿æ¬„ä½å­˜åœ¨
    required_cols = ['order_date', 'order_sn', 'buyer_username']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        logging.error(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_cols}")
        return df
    
    # æ¸…ç†è¨‚å–®æ—¥æœŸ
    order_date = df['order_date'].astype(str).str.strip()
    order_date = order_date.replace(['nan', 'NaN', '<NA>', 'None', ''], 'NO_DATE')
    
    # æ¸…ç†è¨‚å–®ç·¨è™Ÿ
    order_sn = df['order_sn'].fillna('').astype(str).str.strip()
    order_sn = order_sn.replace(['nan', 'NaN', '<NA>', 'None'], 'NO_ORDER')
    
    # æ¸…ç†è²·å®¶å¸³è™Ÿ
    buyer_username = df['buyer_username'].fillna('').astype(str).str.strip()
    buyer_username = buyer_username.replace(['nan', 'NaN', '<NA>', 'None'], 'NO_BUYER')
    
    # å»ºç«‹è¤‡åˆä¸»éµï¼šorder_date + order_sn + buyer_username
    df['composite_key'] = order_date + '|||' + order_sn + '|||' + buyer_username
    
    # é™¤éŒ¯ï¼šè¨˜éŒ„ä¸»éµç”Ÿæˆçµ±è¨ˆ
    unique_keys = df['composite_key'].nunique()
    total_keys = len(df)
    duplicate_count = total_keys - unique_keys
    
    logging.info(f"è¤‡åˆä¸»éµç”Ÿæˆçµ±è¨ˆ: ç¸½æ•¸={total_keys}, å”¯ä¸€è¨‚å–®={unique_keys}, åŒè¨‚å–®å¤šå•†å“={duplicate_count}")
    
    if duplicate_count > 0:
        print(f"ğŸ“Š è¨‚å–®çµ±è¨ˆ: {total_keys} ç­†è¨˜éŒ„å°æ‡‰ {unique_keys} å€‹å”¯ä¸€è¨‚å–®")
        print(f"    å¹³å‡æ¯è¨‚å–® {total_keys/unique_keys:.1f} å€‹å•†å“é …ç›®")
        
        # é¡¯ç¤ºä¸€äº›ç¯„ä¾‹ï¼ˆåŒä¸€è¨‚å–®çš„å¤šå€‹å•†å“ï¼‰
        duplicates = df[df.duplicated('composite_key', keep=False)]
        if len(duplicates) > 0:
            sample_order = duplicates['composite_key'].iloc[0]
            sample_items = df[df['composite_key'] == sample_order]
            print(f"    ç¯„ä¾‹è¨‚å–® {sample_order.split('|||')[1]} åŒ…å« {len(sample_items)} å€‹å•†å“é …ç›®")
    
    return df


def update_logic_with_order_level_replacement(df_old, df_new):
    """ä»¥è¨‚å–®ç‚ºå–®ä½é€²è¡Œè¦†è“‹æ›´æ–°çš„é‚è¼¯"""
    
    if df_old.empty:
        print("ğŸ“ é¦–æ¬¡å»ºç«‹ä¸»æª”")
        return df_new, pd.DataFrame()
    
    print("ğŸ”„ é€²è¡Œä»¥è¨‚å–®ç‚ºå–®ä½çš„è³‡æ–™æ¯”å°èˆ‡æ›´æ–°...")
    
    # ç¢ºå®šæ–°è³‡æ–™çš„æ—¥æœŸç¯„åœ
    new_date_range = None
    if 'order_date' in df_new.columns:
        valid_new_dates = pd.to_datetime(df_new['order_date'], errors='coerce').dropna()
        if len(valid_new_dates) > 0:
            new_date_min = valid_new_dates.min().date()
            new_date_max = valid_new_dates.max().date()
            new_date_range = (new_date_min, new_date_max)
            print(f"   -> æ–°è³‡æ–™æ—¥æœŸç¯„åœ: {new_date_min} åˆ° {new_date_max}")
            logging.info(f"æ–°è³‡æ–™æ—¥æœŸç¯„åœ: {new_date_min} åˆ° {new_date_max}")
    
    # å»ºç«‹è¤‡åˆä¸»éµ
    df_old = create_robust_composite_key(df_old)
    df_new = create_robust_composite_key(df_new)
    
    # å–å¾—æ‰€æœ‰å”¯ä¸€çš„è¨‚å–®ä¸»éµ
    old_order_keys = set(df_old['composite_key'].unique())
    new_order_keys = set(df_new['composite_key'].unique())
    
    # æ‰¾å‡ºåœ¨æ–°è³‡æ–™æ—¥æœŸç¯„åœå…§çš„èˆŠè¨‚å–®
    if new_date_range and 'order_date' in df_old.columns:
        # ç¢ºä¿ order_date æ˜¯æ—¥æœŸæ ¼å¼
        df_old['order_date_parsed'] = pd.to_datetime(df_old['order_date'], errors='coerce').dt.date
        
        # æ‰¾å‡ºåœ¨æ–°è³‡æ–™æ—¥æœŸç¯„åœå…§çš„èˆŠè¨‚å–®
        mask_in_range = (
            (df_old['order_date_parsed'] >= new_date_range[0]) & 
            (df_old['order_date_parsed'] <= new_date_range[1])
        )
        old_orders_in_range = set(df_old.loc[mask_in_range, 'composite_key'].unique())
        old_orders_outside_range = set(df_old.loc[~mask_in_range, 'composite_key'].unique())
        
        print(f"   -> æ—¥æœŸç¯„åœå…§çš„èˆŠè¨‚å–®: {len(old_orders_in_range)} å€‹")
        print(f"   -> æ—¥æœŸç¯„åœå¤–çš„èˆŠè¨‚å–®: {len(old_orders_outside_range)} å€‹")
        
        # æ‰¾å‡ºè¢«æ–°è³‡æ–™è¦†è“‹çš„è¨‚å–®ï¼ˆåœ¨ç¯„åœå…§ä¸”æœ‰æ–°ç‰ˆæœ¬ï¼‰
        orders_to_replace = old_orders_in_range & new_order_keys
        
        # æ‰¾å‡ºåœ¨ç¯„åœå…§ä½†æ–°è³‡æ–™ä¸­æ¶ˆå¤±çš„è¨‚å–®ï¼ˆçœŸæ­£çš„å­¤å…’è¨‚å–®ï¼‰
        orphaned_orders = old_orders_in_range - new_order_keys
        
        # ä¿ç•™çš„èˆŠè³‡æ–™ï¼šæ—¥æœŸç¯„åœå¤–çš„ + ç¯„åœå…§ä½†æœªè¢«è¦†è“‹çš„
        orders_to_keep = old_orders_outside_range | (old_orders_in_range - new_order_keys - orders_to_replace)
        
        print(f"   -> è¢«æ–°è³‡æ–™è¦†è“‹çš„è¨‚å–®: {len(orders_to_replace)} å€‹")
        print(f"   -> æ¶ˆå¤±çš„è¨‚å–®ï¼ˆå­¤å…’ï¼‰: {len(orphaned_orders)} å€‹")
        print(f"   -> ä¿ç•™çš„èˆŠè¨‚å–®: {len(orders_to_keep)} å€‹")
        
    else:
        # å¦‚æœç„¡æ³•ç¢ºå®šæ—¥æœŸç¯„åœï¼Œä½¿ç”¨å…¨åŸŸæ¯”å°æ¨¡å¼
        logging.warning("ç„¡æ³•ç¢ºå®šæ–°è³‡æ–™æ—¥æœŸç¯„åœï¼Œä½¿ç”¨å…¨åŸŸæ¯”å°æ¨¡å¼")
        print("   -> âš ï¸ ç„¡æ³•ç¢ºå®šæ—¥æœŸç¯„åœï¼Œä½¿ç”¨å…¨åŸŸæ¯”å°æ¨¡å¼")
        
        orders_to_replace = old_order_keys & new_order_keys
        orphaned_orders = old_order_keys - new_order_keys
        orders_to_keep = old_order_keys - new_order_keys - orders_to_replace
    
    # ç¯©é¸è³‡æ–™
    df_old_kept = df_old[df_old['composite_key'].isin(orders_to_keep)]
    orphaned_records = df_old[df_old['composite_key'].isin(orphaned_orders)]
    df_new_kept = df_new  # æ–°è³‡æ–™å…¨éƒ¨ä¿ç•™
    
    # çµ±è¨ˆè¨‚å–®å±¤ç´šçš„è®ŠåŒ–
    old_order_count = len(df_old_kept['composite_key'].unique()) if not df_old_kept.empty else 0
    new_order_count = len(df_new_kept['composite_key'].unique()) if not df_new_kept.empty else 0
    orphan_order_count = len(orphaned_records['composite_key'].unique()) if not orphaned_records.empty else 0
    
    print(f"   -> æœ€çµ‚çµæœ:")
    print(f"      ä¿ç•™èˆŠè¨‚å–®: {old_order_count} å€‹ ({len(df_old_kept)} ç­†è¨˜éŒ„)")
    print(f"      æ–°å¢/æ›´æ–°è¨‚å–®: {new_order_count} å€‹ ({len(df_new_kept)} ç­†è¨˜éŒ„)")
    print(f"      å­¤å…’è¨‚å–®: {orphan_order_count} å€‹ ({len(orphaned_records)} ç­†è¨˜éŒ„)")
    
    # æ”¶é›†æ‰€æœ‰æ¬„ä½ï¼Œä¿è­‰æ¬„ä½å®Œæ•´æ€§
    all_cols = sorted(set(df_old_kept.columns) | set(df_new_kept.columns))
    df_old_aligned = df_old_kept.reindex(columns=all_cols)
    df_new_aligned = df_new_kept.reindex(columns=all_cols)
    
    # å»ºç«‹ dtype æ˜ å°„
    dtype_map = {}
    for col in all_cols:
        if col in df_old_kept.columns and not df_old_kept[col].isna().all():
            dtype_map[col] = df_old_kept[col].dtype
        elif col in df_new_kept.columns and not df_new_kept[col].isna().all():
            dtype_map[col] = df_new_kept[col].dtype
        else:
            dtype_map[col] = object
    
    # å¥—ç”¨ dtype
    try:
        df_old_aligned = df_old_aligned.astype(dtype_map, errors='ignore')
        df_new_aligned = df_new_aligned.astype(dtype_map, errors='ignore')
    except Exception as e:
        logging.warning(f"è³‡æ–™é¡å‹è½‰æ›æ™‚ç™¼ç”Ÿè­¦å‘Š: {e}")
    
    # åˆä½µ
    final_master_df = pd.concat([df_old_aligned, df_new_aligned], ignore_index=True)
    
    return final_master_df, orphaned_records


def clean_column_names(df):
    """æ¸…ç†æ¬„ä½åç¨±ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šé¤˜ç©ºç™½"""
    # æ¸…ç†æ¬„ä½åç¨±
    cleaned_columns = {}
    for col in df.columns:
        # ç§»é™¤æ›è¡Œç¬¦ã€å¤šé¤˜ç©ºç™½ï¼Œä¸¦ä¿ç•™ä¸»è¦å…§å®¹
        cleaned_col = col.replace('\n', '').replace('\r', '').strip()
        
        # è™•ç†ç‰¹æ®Šçš„é•·æ¬„ä½åç¨±
        if 'è‹¥æ‚¨æ˜¯è‡ªè¡Œé…é€è«‹ä½¿ç”¨å¾Œæ–¹è¦çš®å°ˆç·šå’ŒåŒ…è£¹æŸ¥è©¢ç¢¼è¯ç¹«è²·å®¶' in cleaned_col:
            cleaned_col = 'æ”¶ä»¶è€…é›»è©±'
        elif 'è«‹è¤‡è£½ä¸‹æ–¹å®Œæ•´ç·¨è™Ÿæä¾›çµ¦æ‚¨é…åˆçš„ç‰©æµå•†ç•¶åšè¯çµ¡é›»è©±' in cleaned_col:
            cleaned_col = 'è¦çš®å°ˆç·šå’ŒåŒ…è£¹æŸ¥è©¢ç¢¼'
        
        cleaned_columns[col] = cleaned_col
    
    df.rename(columns=cleaned_columns, inplace=True)
    
    # è¨˜éŒ„æ¸…ç†çµæœ
    logging.info(f"æ¬„ä½åç¨±æ¸…ç†å®Œæˆï¼Œæ¸…ç†äº† {len([k for k, v in cleaned_columns.items() if k != v])} å€‹æ¬„ä½")
    
    return df


def parse_order_date_from_sn(order_sn):
    """å¾è¨‚å–®ç·¨è™Ÿè§£æè¨‚å–®æ—¥æœŸ"""
    try:
        if pd.isna(order_sn) or order_sn == '':
            return None
        
        order_sn_str = str(order_sn).strip()
        
        # è¦çš®è¨‚å–®ç·¨è™Ÿé€šå¸¸æ ¼å¼ç‚º: YYMMDDXXXXXXXX (å‰6ä½æ˜¯æ—¥æœŸ)
        if len(order_sn_str) >= 6:
            date_part = order_sn_str[:6]
            # å˜—è©¦è§£æç‚º YYMMDD æ ¼å¼
            try:
                parsed_date = pd.to_datetime(date_part, format='%y%m%d')
                return parsed_date.date()
            except:
                # å¦‚æœå¤±æ•—ï¼Œå˜—è©¦å…¶ä»–å¯èƒ½çš„æ ¼å¼
                try:
                    # æœ‰äº›å¯èƒ½æ˜¯ YYYYMMDD æ ¼å¼
                    if len(order_sn_str) >= 8:
                        date_part_8 = order_sn_str[:8]
                        parsed_date = pd.to_datetime(date_part_8, format='%Y%m%d')
                        return parsed_date.date()
                except:
                    pass
        
        return None
    except Exception as e:
        logging.warning(f"è§£æè¨‚å–®ç·¨è™Ÿæ—¥æœŸæ™‚ç™¼ç”ŸéŒ¯èª¤: {order_sn}, éŒ¯èª¤: {e}")
        return None


def load_and_clean_new_data():
    """å¾ Excel æª”æ¡ˆè®€å–ã€è§£æã€æ¸…ç†ä¸¦è½‰æ›æ‰€æœ‰æ–°è¨‚å–®è³‡æ–™ã€‚"""
    logging.info("Starting to load and clean new data from Excel files.")
    files_to_process = glob.glob(os.path.join(INPUT_DIR, '*.xlsx'))
    if not files_to_process:
        logging.warning("No new Excel files found in input directory.")
        return None, []

    all_dataframes = []
    processed_files_paths = []
    logging.info(f"Found {len(files_to_process)} file(s) to process.")
    print(f"ğŸ” ç™¼ç¾ {len(files_to_process)} å€‹æ–°æª”æ¡ˆï¼Œé–‹å§‹è§£æ...")

    for filepath in files_to_process:
        filename = os.path.basename(filepath)
        try:
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            os.makedirs(ARCHIVE_DIR, exist_ok=True)
            
            # è§£æåº—é‹ªè³‡è¨Š
            anchor_pattern = '_Order.all.'
            first_underscore_pos = filename.find('_')
            anchor_pos = filename.find(anchor_pattern)
            if first_underscore_pos == -1 or anchor_pos == -1 or anchor_pos < first_underscore_pos:
                raise ValueError(f"æª”åæ ¼å¼ä¸ç¬¦ï¼Œç¼ºå°‘åº—é‹ªè³‡è¨Šæˆ– '{anchor_pattern}' æ¨™è¨˜")
            
            shop_name = filename[:first_underscore_pos]
            shop_account = filename[first_underscore_pos + 1:anchor_pos]
            if not shop_name or not shop_account:
                raise ValueError("åº—é‹ªåç¨±æˆ–å¸³è™Ÿç‚ºç©º")

            # è®€å– Excel æª”æ¡ˆ
            print(f"   -> ğŸ“– è®€å–æª”æ¡ˆ: {filename}")
            df = pd.read_excel(filepath, dtype=str)
            
            # æ¸…ç†æ¬„ä½åç¨±
            df = clean_column_names(df)
            
            # é¡¯ç¤ºå¯¦éš›è®€å–åˆ°çš„æ¬„ä½ï¼ˆç”¨æ–¼é™¤éŒ¯ï¼‰
            logging.info(f"æª”æ¡ˆ {filename} æ¸…ç†å¾Œçš„æ¬„ä½: {list(df.columns)}")
            
            # é‡å‘½åæ¬„ä½å‰ï¼Œæª¢æŸ¥æ˜ å°„
            unmapped_columns = [col for col in df.columns if col not in COLUMN_MAPPING]
            if unmapped_columns:
                logging.warning(f"æª”æ¡ˆ {filename} æœ‰æœªæ˜ å°„çš„æ¬„ä½: {unmapped_columns}")
                print(f"   -> âš ï¸ æœªæ˜ å°„æ¬„ä½: {unmapped_columns}")
            
            # é‡å‘½åæ¬„ä½
            df.rename(columns=COLUMN_MAPPING, inplace=True)
            
            # çµ±è¨ˆæˆåŠŸæ˜ å°„çš„æ¬„ä½æ•¸é‡
            mapped_count = len([col for col in df.columns if col in COLUMN_MAPPING.values()])
            print(f"   -> ğŸ“‹ æˆåŠŸæ˜ å°„ {len(COLUMN_MAPPING)} å€‹æ¬„ä½ï¼Œè³‡æ–™åŒ…å« {len(df.columns)} å€‹æ¬„ä½")
            
            # æª¢æŸ¥å¿…è¦æ¬„ä½æ˜¯å¦å­˜åœ¨
            required_fields = ['order_sn', 'buyer_username']
            missing_fields = [field for field in required_fields if field not in df.columns]
            if missing_fields:
                logging.warning(f"æª”æ¡ˆ {filename} ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_fields}")
                print(f"   -> âš ï¸ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_fields}")
            else:
                print(f"   -> âœ… æ‰€æœ‰å¿…è¦æ¬„ä½éƒ½å­˜åœ¨")
            
            # æ¸…ç†æ–‡å­—æ¬„ä½ä¸­çš„æ›è¡Œç¬¦
            for col in df.select_dtypes(include=['object']).columns:
                if col in df.columns:
                    df[col] = df[col].astype(str).str.replace('\n', ' ', regex=False).str.replace('\r', '', regex=False)

            # æ–°å¢åº—é‹ªè³‡è¨Šå’Œè™•ç†æ—¥æœŸ
            df['shop_name'] = shop_name
            df['shop_account'] = shop_account
            df['processing_date'] = datetime.now().date()

            # è½‰æ›æ•¸å€¼æ¬„ä½
            float_columns = [
                'product_total_price', 'buyer_paid_shipping_fee', 'shopee_shipping_subsidy',
                'return_shipping_fee', 'total_amount_paid_by_buyer', 'shopee_subsidy_amount',
                'shopee_coin_offset', 'credit_card_promotion_discount', 'transaction_fee',
                'other_service_fee', 'payment_processing_fee', 'product_original_price',
                'product_campaign_price'
            ]
            for col in float_columns:
                if col in df.columns:
                    # ç§»é™¤å¯èƒ½çš„è²¨å¹£ç¬¦è™Ÿå’Œé€—è™Ÿ
                    df[col] = df[col].astype(str).str.replace(r'[^\d.-]', '', regex=True)
                    df[col] = pd.to_numeric(df[col], errors='coerce')

            # è™•ç†è²»ç‡æ¬„ä½ï¼ˆç™¾åˆ†æ¯”è½‰å°æ•¸ï¼‰
            if 'payment_processing_fee_rate' in df.columns:
                df['payment_processing_fee_rate'] = (
                    pd.to_numeric(
                        df['payment_processing_fee_rate'].astype(str)
                        .str.replace('%', '', regex=False)
                        .str.replace(r'[^\d.-]', '', regex=True),
                        errors='coerce'
                    ).fillna(0) / 100
                )

            # è§£æè¨‚å–®æ—¥æœŸ
            if 'order_sn' in df.columns:
                print(f"   -> ğŸ“… è§£æè¨‚å–®æ—¥æœŸ...")
                df['order_date'] = df['order_sn'].apply(parse_order_date_from_sn)
                # çµ±è¨ˆè§£ææˆåŠŸçš„æ•¸é‡
                valid_dates = df['order_date'].notna().sum()
                print(f"      æˆåŠŸè§£æ {valid_dates}/{len(df)} ç­†è¨‚å–®æ—¥æœŸ")
                logging.info(f"è¨‚å–®æ—¥æœŸè§£æ: {valid_dates}/{len(df)} æˆåŠŸ")

            # è™•ç†æ—¥æœŸæ¬„ä½
            if 'ship_by_date' in df.columns:
                df['ship_by_date'] = pd.to_datetime(df['ship_by_date'], errors='coerce').dt.date

            # è™•ç†æ™‚é–“æˆ³æ¬„ä½
            timestamp_cols = [
                'order_creation_timestamp', 'buyer_payment_timestamp',
                'actual_shipping_timestamp', 'order_completion_timestamp'
            ]
            for col in timestamp_cols:
                if col in df.columns:
                    # è™•ç†ç©ºå€¼å’Œ '-' ç¬¦è™Ÿ
                    df[col] = df[col].replace(['-', '', 'nan', 'NaN'], pd.NaT)
                    df[col] = pd.to_datetime(df[col], errors='coerce')

            # è™•ç†æ•´æ•¸æ¬„ä½
            int_columns = ['quantity', 'return_quantity', 'installment_plan_periods', 'days_to_ship']
            for col in int_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')

            all_dataframes.append(df)
            processed_files_paths.append(filepath)
            logging.info(f"Successfully parsed and cleaned: {filename}")
            print(f"   -> âœ… å·²è§£æ: {filename} ({len(df)} ç­†è³‡æ–™)")
            
        except Exception as e:
            logging.error(f"Failed to parse {filename}: {e}\n{traceback.format_exc()}")
            print(f"   -> âŒ è§£æå¤±æ•—: {filename}ï¼ŒéŒ¯èª¤: {e}")

    if not all_dataframes:
        logging.warning("No dataframes were created from Excel files.")
        return None, []

    final_df = pd.concat(all_dataframes, ignore_index=True)
    logging.info(f"Concatenated all dataframes. Total new rows: {len(final_df)}")
    print(f"ğŸ“Š åˆä½µå®Œæˆï¼Œå…± {len(final_df)} ç­†æ–°è³‡æ–™")
    return final_df, processed_files_paths


def run_update_logic():
    """ä¸»æµç¨‹ï¼šåŸ·è¡Œè®€å–ã€æ¯”å°ã€æ›´æ–°ã€æ­¸æª”çš„å®Œæ•´é‚è¼¯"""
    logging.info("Starting main update logic.")
    df_new, processed_files = load_and_clean_new_data()
    if df_new is None:
        print("ğŸŸ¡ åœ¨ 'input' è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ–°æª”æ¡ˆå¯è™•ç†ã€‚")
        logging.info("No new data to process. Exiting.")
        return

    if os.path.exists(OUTPUT_CSV_PATH):
        logging.info(f"Loading existing master file from {OUTPUT_CSV_PATH}")
        print(f"\nğŸ“‘ æ­£åœ¨è®€å–ç¾æœ‰ä¸»æª”: {os.path.basename(OUTPUT_CSV_PATH)}")
        try:
            df_old = pd.read_csv(OUTPUT_CSV_PATH, dtype=str)
            print(f"   -> è¼‰å…¥ {len(df_old)} ç­†ç¾æœ‰è³‡æ–™")
        except Exception as e:
            logging.error(f"è®€å–ç¾æœ‰ä¸»æª”å¤±æ•—: {e}")
            print(f"   -> âŒ è®€å–ä¸»æª”å¤±æ•—: {e}")
            df_old = pd.DataFrame()
    else:
        logging.info("No existing master file found.")
        print("\nğŸ“‘ æœªç™¼ç¾ç¾æœ‰ä¸»æª”ï¼Œå°‡ç›´æ¥å»ºç«‹æ–°æª”æ¡ˆã€‚")
        df_old = pd.DataFrame()

    # è½‰æ›èˆŠè³‡æ–™çš„è³‡æ–™é¡å‹
    if not df_old.empty:
        date_cols = ['processing_date', 'order_date', 'ship_by_date']
        for col in date_cols:
            if col in df_old.columns:
                df_old[col] = pd.to_datetime(df_old[col], errors='coerce').dt.date

        timestamp_cols = [
            'order_creation_timestamp', 'buyer_payment_timestamp',
            'actual_shipping_timestamp', 'order_completion_timestamp'
        ]
        for col in timestamp_cols:
            if col in df_old.columns:
                df_old[col] = pd.to_datetime(df_old[col], errors='coerce')

    # ===== ä½¿ç”¨æ–°çš„è¨‚å–®å±¤ç´šè¦†è“‹é‚è¼¯ =====
    final_master_df, orphaned_records = update_logic_with_order_level_replacement(df_old, df_new)
    
    # ç§»é™¤è‡¨æ™‚æ¬„ä½
    final_master_df = final_master_df.drop(columns=['composite_key', 'order_date_parsed'], errors='ignore')
    orphaned_records = orphaned_records.drop(columns=['composite_key', 'order_date_parsed'], errors='ignore')
    
    # ç¢ºä¿æ¬„ä½é †åºæ­£ç¢º
    available_columns = [col for col in FINAL_COLUMN_ORDER if col in final_master_df.columns]
    final_master_df = final_master_df.reindex(columns=available_columns)

    # å„²å­˜èˆ‡æ­¸æª”æµç¨‹
    logging.info(f"Saving final master dataframe with {len(final_master_df)} rows to CSV.")
    print("\nğŸ’¾ æ­£åœ¨å„²å­˜æ›´æ–°å¾Œçš„ä¸»æª”...")
    os.makedirs(os.path.dirname(OUTPUT_CSV_PATH), exist_ok=True)
    final_master_df.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8-sig')
    print(f"   -> âœ… ä¸»æª”å·²æˆåŠŸæ›´æ–°ä¸¦å„²å­˜è‡³: {os.path.basename(OUTPUT_CSV_PATH)} ({len(final_master_df)} ç­†ç´€éŒ„)")

    if not orphaned_records.empty:
        logging.info(f"Found {len(orphaned_records)} orphaned records. Saving to orphan file.")
        print(f"\nğŸŸ¡ ç™¼ç¾ {len(orphaned_records)} ç­†å·²æ¶ˆå¤±çš„è¨‚å–®ï¼Œæ­£åœ¨å­˜æª”...")
        orphaned_records['orphaned_timestamp'] = datetime.now()
        is_first_write = not os.path.exists(ORPHAN_CSV_PATH)
        os.makedirs(os.path.dirname(ORPHAN_CSV_PATH), exist_ok=True)
        orphaned_records.to_csv(ORPHAN_CSV_PATH, mode='a', index=False, header=is_first_write, encoding='utf-8-sig')
        print(f"   -> âœ… å·²é™„åŠ è‡³: {os.path.basename(ORPHAN_CSV_PATH)}")
    else:
        logging.info("No orphaned records found this run.")
        print("\nğŸŸ¢ æœ¬æ¬¡æ›´æ–°ç¯„åœå…§ç„¡ä»»ä½•å·²æ¶ˆå¤±çš„è¨‚å–®ã€‚")

    logging.info("Archiving processed source files.")
    print("\nğŸ—„ï¸  æ­£åœ¨æ­¸æª”å·²è™•ç†çš„åŸå§‹æª”æ¡ˆ...")
    os.makedirs(ARCHIVE_DIR, exist_ok=True)
    for filepath in processed_files:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = os.path.basename(filepath)
        archive_filename = f"{os.path.splitext(base_filename)[0]}_{timestamp}{os.path.splitext(base_filename)[1]}"
        archive_path = os.path.join(ARCHIVE_DIR, archive_filename)
        shutil.move(filepath, archive_path)
        print(f"   -> ğŸ“¦ {base_filename} â†’ {archive_filename}")
    logging.info(f"Archived {len(processed_files)} files.")
    print(f"   -> âœ… å·²æˆåŠŸæ­¸æª” {len(processed_files)} å€‹æª”æ¡ˆã€‚")


if __name__ == "__main__":
    try:
        logging.info("================ SCRIPT START ================")
        print("ğŸš€ é–‹å§‹åŸ·è¡Œè¦çš®è¨‚å–® ETL è™•ç†æµç¨‹...")
        run_update_logic()
        logging.info("================ SCRIPT SUCCESS ================")
        print("\nğŸ‰ æ‰€æœ‰æœ¬åœ°ç«¯ CSV æ¸…æ´—æµç¨‹åŸ·è¡Œå®Œç•¢ï¼")
    except Exception as e:
        error_message = traceback.format_exc()
        logging.error(f"An unhandled exception occurred:\n{error_message}")
        print(f"\nâŒ ç™¼ç”Ÿæœªé æœŸçš„åš´é‡éŒ¯èª¤ï¼š{e}")
        print(f"è©³ç´°éŒ¯èª¤è³‡è¨Šè«‹æª¢æŸ¥ 'python_script_log.txt' æª”æ¡ˆã€‚")
        raise