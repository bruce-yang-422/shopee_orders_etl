#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BigQueryæ‰¹é‡CSVæª”æ¡ˆä¸Šå‚³è…³æœ¬
è‡ªå‹•ä¸Šå‚³å¤šå€‹Shopeeè¨‚å–®CSVæª”æ¡ˆè‡³å°æ‡‰çš„BigQueryè¡¨æ ¼

æª”æ¡ˆä½ç½®: C:/Users/user/Documents/shopee_orders_etl/scripts/batch_upload_to_bq.py
"""

import os
import sys
import pandas as pd
from google.cloud import bigquery
from google.cloud.exceptions import NotFound, Conflict
from google.oauth2 import service_account
import logging
from datetime import datetime
from typing import Optional, Dict, List
from pathlib import Path

# è¨­å®šæ—¥èªŒè¨˜éŒ„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('bigquery_batch_upload.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class BigQueryBatchUploader:
    """BigQueryæ‰¹é‡CSVä¸Šå‚³å·¥å…·é¡"""
    
    def __init__(self, 
                 credentials_path: str,
                 project_id: str,
                 dataset_id: str = "shopee_data"):
        """
        åˆå§‹åŒ–BigQueryæ‰¹é‡ä¸Šå‚³å™¨
        
        Args:
            credentials_path: æœå‹™å¸³æˆ¶æ†‘è­‰JSONæª”æ¡ˆè·¯å¾‘
            project_id: Google Cloudå°ˆæ¡ˆID
            dataset_id: BigQueryè³‡æ–™é›†ID
        """
        self.credentials_path = credentials_path
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.client = None
        self.upload_results = []
        
    def initialize_client(self) -> bool:
        """åˆå§‹åŒ–BigQueryå®¢æˆ¶ç«¯"""
        try:
            # æª¢æŸ¥æ†‘è­‰æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.credentials_path):
                logger.error(f"æ†‘è­‰æª”æ¡ˆä¸å­˜åœ¨: {self.credentials_path}")
                return False
            
            # å»ºç«‹æ†‘è­‰
            credentials = service_account.Credentials.from_service_account_file(
                self.credentials_path,
                scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )
            
            # åˆå§‹åŒ–BigQueryå®¢æˆ¶ç«¯
            self.client = bigquery.Client(
                credentials=credentials,
                project=self.project_id
            )
            
            logger.info("BigQueryå®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–BigQueryå®¢æˆ¶ç«¯å¤±æ•—: {str(e)}")
            return False
    
    def create_dataset_if_not_exists(self) -> bool:
        """å»ºç«‹è³‡æ–™é›†ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        try:
            dataset_ref = self.client.dataset(self.dataset_id)
            
            try:
                self.client.get_dataset(dataset_ref)
                logger.info(f"è³‡æ–™é›† {self.dataset_id} å·²å­˜åœ¨")
                return True
            except NotFound:
                # è³‡æ–™é›†ä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°çš„
                dataset = bigquery.Dataset(dataset_ref)
                dataset.location = "asia-east1"  # å°ç£åœ°å€
                dataset.description = "Shopeeè¨‚å–®è³‡æ–™"
                
                dataset = self.client.create_dataset(dataset)
                logger.info(f"å·²å»ºç«‹è³‡æ–™é›†: {self.dataset_id}")
                return True
                
        except Exception as e:
            logger.error(f"å»ºç«‹è³‡æ–™é›†å¤±æ•—: {str(e)}")
            return False
    
    def read_csv_file(self, csv_path: str) -> Optional[pd.DataFrame]:
        """è®€å–CSVæª”æ¡ˆ"""
        try:
            if not os.path.exists(csv_path):
                logger.warning(f"CSVæª”æ¡ˆä¸å­˜åœ¨ï¼Œè·³é: {csv_path}")
                return None
            
            # è®€å–CSVæª”æ¡ˆ
            df = pd.read_csv(csv_path, encoding='utf-8')
            logger.info(f"æˆåŠŸè®€å– {Path(csv_path).name}ï¼Œå…± {len(df)} ç­†è³‡æ–™ï¼Œ{len(df.columns)} å€‹æ¬„ä½")
            
            return df
            
        except Exception as e:
            logger.error(f"è®€å–CSVæª”æ¡ˆå¤±æ•— {csv_path}: {str(e)}")
            return None
    
    def upload_dataframe_to_table(self, 
                                 df: pd.DataFrame, 
                                 table_id: str, 
                                 csv_filename: str,
                                 write_disposition: str = "WRITE_TRUNCATE") -> bool:
        """
        ä¸Šå‚³DataFrameåˆ°æŒ‡å®šçš„BigQueryè¡¨æ ¼
        
        Args:
            df: è¦ä¸Šå‚³çš„DataFrame
            table_id: BigQueryè¡¨æ ¼ID
            csv_filename: CSVæª”æ¡ˆåç¨±ï¼ˆç”¨æ–¼æ—¥èªŒï¼‰
            write_disposition: å¯«å…¥æ¨¡å¼
        """
        try:
            # è¨­å®šè¡¨æ ¼åƒè€ƒ
            table_ref = self.client.dataset(self.dataset_id).table(table_id)
            
            # è¨­å®šä¸Šå‚³è¨­å®š
            job_config = bigquery.LoadJobConfig()
            job_config.source_format = bigquery.SourceFormat.CSV
            job_config.autodetect = True  # è‡ªå‹•åµæ¸¬schema
            job_config.write_disposition = write_disposition
            
            logger.info(f"é–‹å§‹ä¸Šå‚³ {csv_filename} ({len(df)} ç­†è³‡æ–™) åˆ° {self.project_id}.{self.dataset_id}.{table_id}")
            
            # é–‹å§‹ä¸Šå‚³
            job = self.client.load_table_from_dataframe(
                df, 
                table_ref, 
                job_config=job_config
            )
            
            # ç­‰å¾…ä½œæ¥­å®Œæˆ
            job.result()
            
            # æª¢æŸ¥çµæœ
            table = self.client.get_table(table_ref)
            logger.info(f"âœ… {csv_filename} ä¸Šå‚³å®Œæˆï¼è¡¨æ ¼ {table_id} ç¾åœ¨æœ‰ {table.num_rows} ç­†è³‡æ–™")
            
            # è¨˜éŒ„ä¸Šå‚³çµæœ
            self.upload_results.append({
                "csv_file": csv_filename,
                "table_id": table_id,
                "status": "æˆåŠŸ",
                "rows_uploaded": len(df),
                "final_rows": table.num_rows,
                "table_size_bytes": table.num_bytes
            })
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¸Šå‚³ {csv_filename} åˆ° {table_id} å¤±æ•—: {str(e)}")
            
            # è¨˜éŒ„å¤±æ•—çµæœ
            self.upload_results.append({
                "csv_file": csv_filename,
                "table_id": table_id,
                "status": "å¤±æ•—",
                "error": str(e),
                "rows_uploaded": 0,
                "final_rows": 0,
                "table_size_bytes": 0
            })
            
            return False
    
    def batch_upload(self, file_table_mapping: Dict[str, str], csv_directory: str) -> Dict:
        """
        æ‰¹é‡ä¸Šå‚³å¤šå€‹CSVæª”æ¡ˆ
        
        Args:
            file_table_mapping: CSVæª”æ¡ˆåç¨±å°æ‡‰BigQueryè¡¨æ ¼åç¨±çš„å­—å…¸
            csv_directory: CSVæª”æ¡ˆæ‰€åœ¨ç›®éŒ„
            
        Returns:
            ä¸Šå‚³çµæœæ‘˜è¦
        """
        logger.info(f"=== é–‹å§‹æ‰¹é‡ä¸Šå‚³ {len(file_table_mapping)} å€‹æª”æ¡ˆ ===")
        
        success_count = 0
        skip_count = 0
        fail_count = 0
        
        for csv_filename, table_id in file_table_mapping.items():
            csv_path = os.path.join(csv_directory, csv_filename)
            
            logger.info(f"\n--- è™•ç†æª”æ¡ˆ: {csv_filename} â†’ {table_id} ---")
            
            # è®€å–CSVæª”æ¡ˆ
            df = self.read_csv_file(csv_path)
            
            if df is None:
                skip_count += 1
                self.upload_results.append({
                    "csv_file": csv_filename,
                    "table_id": table_id,
                    "status": "è·³é",
                    "error": "æª”æ¡ˆä¸å­˜åœ¨",
                    "rows_uploaded": 0,
                    "final_rows": 0,
                    "table_size_bytes": 0
                })
                continue
            
            # ä¸Šå‚³åˆ°BigQuery
            if self.upload_dataframe_to_table(df, table_id, csv_filename):
                success_count += 1
            else:
                fail_count += 1
        
        # è¿”å›æ‘˜è¦
        summary = {
            "total_files": len(file_table_mapping),
            "success": success_count,
            "skipped": skip_count,
            "failed": fail_count,
            "details": self.upload_results
        }
        
        return summary
    
    def print_summary(self, summary: Dict):
        """åˆ—å°ä¸Šå‚³çµæœæ‘˜è¦"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š æ‰¹é‡ä¸Šå‚³çµæœæ‘˜è¦")
        logger.info("="*60)
        logger.info(f"ç¸½æª”æ¡ˆæ•¸: {summary['total_files']}")
        logger.info(f"âœ… æˆåŠŸ: {summary['success']}")
        logger.info(f"â­ï¸  è·³é: {summary['skipped']}")
        logger.info(f"âŒ å¤±æ•—: {summary['failed']}")
        
        logger.info("\nğŸ“‹ è©³ç´°çµæœ:")
        logger.info("-" * 60)
        
        for result in summary['details']:
            status_icon = {"æˆåŠŸ": "âœ…", "è·³é": "â­ï¸", "å¤±æ•—": "âŒ"}.get(result['status'], "â“")
            
            if result['status'] == "æˆåŠŸ":
                size_mb = result['table_size_bytes'] / (1024 * 1024)
                logger.info(f"{status_icon} {result['csv_file']} â†’ {result['table_id']}")
                logger.info(f"    è³‡æ–™ç­†æ•¸: {result['final_rows']:,} | å¤§å°: {size_mb:.1f} MB")
            else:
                logger.info(f"{status_icon} {result['csv_file']} â†’ {result['table_id']}")
                if 'error' in result:
                    logger.info(f"    åŸå› : {result['error']}")

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    
    # è¨­å®šæª”æ¡ˆå’Œè¡¨æ ¼å°æ‡‰é—œä¿‚
    FILE_TABLE_MAPPING = {
        "B01_orders_concat.csv": "b01_orders_concat",
        "B02_order_details.csv": "b02_order_details", 
        "B03_order_simple_details.csv": "b03_order_simple_details",
        "B04_order_shipping_info.csv": "b04_order_shipping_info"
    }
    
    # è¨­å®šè·¯å¾‘
    csv_directory = r"C:\Users\user\Documents\shopee_orders_etl\upload_ready"
    credentials_path = r"C:\Users\user\Documents\shopee_orders_etl\scripts\shopee-etl-reporting-9531f3a7678a.json"
    
    # BigQueryè¨­å®š
    PROJECT_ID = "shopee-etl-reporting"
    DATASET_ID = "shopee_data"
    
    logger.info("ğŸš€ BigQueryæ‰¹é‡CSVä¸Šå‚³é–‹å§‹")
    logger.info(f"â° æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ğŸ“ ä¾†æºç›®éŒ„: {csv_directory}")
    logger.info(f"ğŸ¯ ç›®æ¨™å°ˆæ¡ˆ: {PROJECT_ID}.{DATASET_ID}")
    
    # å»ºç«‹æ‰¹é‡ä¸Šå‚³å™¨
    uploader = BigQueryBatchUploader(
        credentials_path=credentials_path,
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID
    )
    
    # åˆå§‹åŒ–å®¢æˆ¶ç«¯
    if not uploader.initialize_client():
        logger.error("ç„¡æ³•åˆå§‹åŒ–BigQueryå®¢æˆ¶ç«¯ï¼Œç¨‹å¼çµæŸ")
        sys.exit(1)
    
    # å»ºç«‹è³‡æ–™é›†
    if not uploader.create_dataset_if_not_exists():
        logger.error("ç„¡æ³•å»ºç«‹æˆ–å­˜å–è³‡æ–™é›†ï¼Œç¨‹å¼çµæŸ")
        sys.exit(1)
    
    # åŸ·è¡Œæ‰¹é‡ä¸Šå‚³
    summary = uploader.batch_upload(FILE_TABLE_MAPPING, csv_directory)
    
    # åˆ—å°çµæœæ‘˜è¦
    uploader.print_summary(summary)
    
    # æ ¹æ“šçµæœæ±ºå®šç¨‹å¼çµæŸç‹€æ…‹
    if summary['failed'] > 0:
        logger.warning("âš ï¸  éƒ¨åˆ†æª”æ¡ˆä¸Šå‚³å¤±æ•—")
        sys.exit(1)
    elif summary['success'] == 0:
        logger.warning("âš ï¸  æ²’æœ‰ä»»ä½•æª”æ¡ˆæˆåŠŸä¸Šå‚³")
        sys.exit(1)
    else:
        logger.info("ğŸ‰ æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()